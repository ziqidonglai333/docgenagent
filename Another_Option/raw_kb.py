# å¯¼å…¥ä¾èµ–åŒ…
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel,RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import os
from langchain_community.document_loaders import TextLoader
import  streamlit as st
import  streamlit as st

# è®¾ç½®é¡µé¢å¸ƒå±€
st.set_page_config(page_title = "çŸ¥è¯†åº“",
                   page_icon = "ğŸ›„",
                   layout= "wide",
                   initial_sidebar_state="expanded")

# è®¾ç½®æ ‡é¢˜
st.title(body="æ™ºæ°¸æ–¹ç•¥***çŸ¥è¯†åº“***")
# st.markdown(f"<h1 style='color: blue;'>ç‹æ™ºä¸œçš„***çŸ¥è¯†åº“***ğŸ”§</h1>",unsafe_allow_html=True)

# è®¾ç½®å·¦è¾¹æ 
with st.sidebar:
    # è®¾ç½®æ–‡ä»¶ä¸Šä¼ 
    uploaded_file = st.file_uploader("è¯·é€‰æ‹©ä¸Šä¼ çš„æ–‡ä»¶")
    if uploaded_file is not None:
        # stingio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        file_content = uploaded_file.read()
        # contract_content = file_content
        text = file_content.decode("utf-8")
        st.write("æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")

    # åˆ†å‰²çº¿
    st.divider()

    # è®¾ç½®æ¨¡å‹æ¸©åº¦
    temperature = st.slider(label= "æ¸©åº¦",
                            max_value=1.0,
                            min_value=0.0,
                            step= 0.1,
                            value= 0.8)
    st.write(temperature)

    # è®¾ç½®æ£€ç´¢åŒ¹é…çš„æ•°é‡
    select_num = st.slider(label= "æ£€ç´¢æ•°é‡",
                            max_value=20,
                            min_value=1,
                            step= 1,
                            value= 8)
    st.write(select_num)


    # åŠ è½½çŸ¥è¯†
    loader = TextLoader("/root/rag-deom/è´¢åŠ¡åŸºç¡€çŸ¥è¯†(1).txt")
    content = loader.load()

    # å¯¹åŠ è½½çš„çŸ¥è¯†è¿›è¡Œåˆ‡åˆ†å—
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 500,
        chunk_overlap = 10,
        length_function = len,
        add_start_index =True
    )
    documents  = text_splitter.split_documents(content)

    # å¯¹åˆ†å—çš„çŸ¥è¯†è¿›è¡Œå‘é‡åŒ–
    embedding_path ="/root/.cache/modelscope/hub/AI-ModelScope/bge-large-zh-v1.5"
    embeddings = HuggingFaceEmbeddings(model_name = embedding_path)
    vectorstore = Chroma.from_documents(documents = documents,embedding=embeddings)

def retriever_results(query) :
    retriever_results=vectorstore.similarity_search(query=query,k=4)
    knowle = "\n".join(x.page_content for x in retriever_results)
    return(knowle)
    
col1,col2 = st.columns(spec = 2)
with col1:
    input = st.chat_input("è¯·è¾“å…¥ï¼š")
    st.text_area(label = "æ£€ç´¢é—®é¢˜",height = 150,value = input) 

if input is not None:
    retrie_resul = retriever_results(input)
else:
    retrie_resul = ""
    input = ""

with col2:
    st.text_area(label = "æ£€ç´¢å†…å®¹",height = 550,value = (retrie_resul)) 

# å»ºç«‹å¤§æ¨¡å‹å¯¹è±¡é“¾æ¥
from langchain_openai import ChatOpenAI
import os
llm = ChatOpenAI(
    temperature = temperature,
    model="glm-4",
    openai_api_key="d2e482be31c453838f46321a197d117d.XnlnLgeyA0KFR2wE",
    openai_api_base="https://open.bigmodel.cn/api/paas/v4/"
)

prompt  = f"æ ¹æ®{retrie_resul}çš„å†…å®¹ï¼Œå›ç­”{input}é—®é¢˜ã€‚"

from langchain_core.output_parsers import StrOutputParser
output_parser = StrOutputParser()
chain = llm| output_parser
llm_resul = chain.invoke(prompt)

with col1:
    st.text_area(label = "æ¨¡å‹å›å¤",height = 300,value = (llm_resul) )
